{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to infer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import joblib\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.special import expit\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.logger import Config\n",
    "from util.metrics import macro_auc\n",
    "from util.torch import load_model_weights\n",
    "\n",
    "from data.preparation import prepare_data, prepare_folds\n",
    "from model_zoo.models import define_model\n",
    "from inference.predict import infer_onnx, load_sample, infer_sample\n",
    "from params import CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL:\n",
    "    DATA_PATH = \"../input/train_audio/\"\n",
    "else:\n",
    "    DATA_PATH = \"../input/unlabeled_soundscapes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "USE_FP16 = True\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "DURATION = 5\n",
    "SR = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0 if EVAL else \"fullfit_0\"\n",
    "\n",
    "EXP_FOLDERS = [\n",
    "    # (\"../logs/2024-04-12/8/\", [FOLD]),   # LB 0.64 baseline\n",
    "    # (\"../logs/2024-04-18/12/\", [FOLD]),  #\n",
    "    # (\"../logs/2024-04-18/15/\", [FOLD]),  #\n",
    "    # (\"../logs/2024-04-19/4/\", [FOLD]),  # Change norm, sampling\n",
    "    # (\"../logs/2024-04-19/5/\", [FOLD]),  # d=15s\n",
    "    # (\"../logs/2024-04-19/7/\", [FOLD]),  # minmaxnorm, sampling, nocall, less mix\n",
    "    # (\"../logs/2024-04-19/8/\", [FOLD]),  # minmaxnorm, no sampling, nocall, less mix no add\n",
    "    # (\"../logs/2024-04-19/10/\", [FOLD]),  # minmaxnorm more mix more aug\n",
    "    # (\"../logs/2024-04-29/2/\", [FOLD]),  # minmaxnorm fixed crop\n",
    "    # (\"../logs/2024-04-29/4/\", [FOLD]),  # minmaxnorm fixed crop 20s selfmix\n",
    "    # (\"../logs/2024-04-29/6/\", [FOLD]),  # minmaxnorm fixed crop no_xc selfmix\n",
    "    # (\"../logs/2024-04-29/7/\", [FOLD]),  # minmaxnorm fixed crop no_xc selfmix focal_bce ousmk\n",
    "    # (\"../logs/2024-04-30/0/\", [FOLD]),  # minmaxnorm selfmix focal_bce ousmk\n",
    "    # (\"../logs/2024-04-30/1/\", [FOLD]),  # minmaxnorm selfmix focal_bce ousmk++\n",
    "    # (\"../logs/2024-04-30/3/\", [FOLD]),  # minmaxnorm selfmix focal_bce no_xc more mix\n",
    "    # (\"../logs/2024-04-30/4/\", [FOLD]),  # minmaxnorm selfmix bce no_xc ousmk^\n",
    "    # (\"../logs/2024-05-02/0/\", [FOLD]),  # selfmix focal_bce ousmk + dedup, new melspec params, start-end sampling\n",
    "    # (\"../logs/2024-05-02/15/\", [FOLD]),  # no selfmix focal_bce ousmk + 2nd mask, new melspec params, start-end sampling\n",
    "    # (\"../logs/2024-05-04/4/\", [FOLD]),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + 20 epochs, wd AdamW\n",
    "    # (\"../logs/2024-05-04/8/\", [FOLD]),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + 30 epochs, wd AdamW, upsample\n",
    "    # (\"../logs/2024-05-04/9/\", [FOLD]),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + 30 epochs, wd AdamW, upsample less augs\n",
    "    # (\"../logs/2024-05-05/1/\", [FOLD]),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + 20 epochs, wd AdamW new miw\n",
    "    # (\"../logs/2024-05-05/7/\", [FOLD], \"torch\"),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + upsample no ext cls w\n",
    "    # (\"../logs/2024-05-06/1/\", [FOLD]),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + upsample no ext fix mix\n",
    "    # (\"../logs/2024-05-06/2/\", [FOLD]),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + upsample no ext fix mix b0\n",
    "    # (\"../logs/2024-05-06/3/\", [FOLD]),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + upsample no ext fix mix vit-b0\n",
    "    # (\"../logs/2024-05-06/12/\", [0]),  # no selfmix bce + 2nd mask, new melspec params, start-end sampling + upsample no ext fix mix vit-b0\n",
    "    # (\"../logs/2024-05-06/18/\", [FOLD], \"torch\"),  # vit-b0 bce ousmk + 2nd mask, new melspec params, start-end sampling + upsample no ext fix mix 3ch \n",
    "    # (\"../logs/2024-05-06/19/\", [FOLD], \"torch\"),  # v2s bce ousmk + 2nd mask, new melspec params, start-end sampling + upsample no ext fix mix 3ch \n",
    "    # (\"../logs/2024-05-06/20/\", [FOLD], \"torch\"),  # v2s bce ousmk + 2nd mask, new melspec params, start-end sampling + upsample no ext fix mix 3ch \n",
    "    # (\"../logs/2024-05-06/22/\", [FOLD], \"torch\"),  # vit-b1\n",
    "    # (\"../logs/2024-05-07/0/\", [FOLD], \"torch\"),  # vit-b0\n",
    "    # (\"../logs/2024-05-07/1/\", [f\"fullfit_{i}\" for i in range(4)], \"torch\"),  # vit-b0 bs32\n",
    "    # (\"../logs/2024-05-07/4/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b0 bs64\n",
    "    # (\"../logs/2024-05-07/5/\", [f\"{i}\" for i in range(4)], \"torch\"),  # vit-b0 repro 0.64\n",
    "    # (\"../logs/2024-05-07/6/\", [f\"{i}\" for i in range(4)], \"torch\"),  # v2-b0 bs64\n",
    "    # (\"../logs/2024-05-07/7/\", [f\"{i}\" for i in range(4)], \"torch\"),  # vit-b1 bs64\n",
    "    # (\"../logs/2024-05-10/18/\", [f\"fullfit_{i}\" for i in range(4, 5)], \"torch\"),  # vit-b0 PL \n",
    "    # (\"../logs/2024-05-11/10/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b0 PL2\n",
    "    # (\"../logs/2024-05-11/11/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # effnet-b0 PL2\n",
    "    # (\"../logs/2024-05-12/0/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b0 PL0.72\n",
    "    # (\"../logs/2024-05-12/1/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # effnet-b0 PL0.72\n",
    "    # (\"../logs/2024-05-12/2/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b1 PL0.72\n",
    "    (\"../logs/2024-05-13/0/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b0 PLBirdnet\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.logger import upload_to_kaggle\n",
    "\n",
    "upload_to_kaggle(\n",
    "    [f[0] for f in EXP_FOLDERS],\n",
    "    directory=\"../output/dataset_4/\",\n",
    "    dataset_name=\"BirdCLEF 2024 Weights 4\",\n",
    "    update_folders=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL:\n",
    "    df = pd.DataFrame({\"path\": glob.glob(DATA_PATH + \"*/*\")})\n",
    "    df[\"id\"] = df[\"path\"].apply(lambda x: x.split(\"/\")[-1][:-4])\n",
    "\n",
    "    folds = pd.read_csv('../input/folds_4.csv')\n",
    "    folds['id'] = folds['filename'].apply(lambda x: x.split('/')[-1][:-4])\n",
    "    df = df.merge(folds)\n",
    "    df = df[df['fold'] == 0].reset_index(drop=True)\n",
    "\n",
    "    df[\"primary_label\"] = df[\"path\"].apply(lambda x:  x.split('/')[-2])\n",
    "else:\n",
    "    df = pd.DataFrame({\"path\": glob.glob(DATA_PATH + \"*\")})\n",
    "    df[\"id\"] = df[\"path\"].apply(lambda x: x.split(\"/\")[-1][:-4])\n",
    "    \n",
    "    # df[\"duration\"] = df[\"path\"].apply(lambda x: librosa.get_duration(path=x))\n",
    "    # df = df[df[\"duration\"] != 240].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for e in EXP_FOLDERS:\n",
    "    try:\n",
    "        exp_folder, folds, runtime = e\n",
    "    except:\n",
    "        exp_folder, folds = e\n",
    "        runtime = \"torch\"\n",
    "    \n",
    "    config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "    \n",
    "    for fold in folds:\n",
    "        weights = exp_folder + f\"{config.name}_{fold}.pt\"\n",
    "\n",
    "        model = define_model(\n",
    "            config.name,\n",
    "            config.melspec_config,\n",
    "            head=config.head,\n",
    "            aug_config=config.aug_config,\n",
    "            num_classes=config.num_classes,\n",
    "            n_channels=config.n_channels,\n",
    "            drop_rate=config.drop_rate,\n",
    "            drop_path_rate=config.drop_path_rate,\n",
    "            norm=config.norm if hasattr(config, \"norm\") else \"min_max\",\n",
    "            top_db=config.top_db if hasattr(config, \"top_db\") else None,\n",
    "            exportable=config.exportable,\n",
    "            verbose=True,\n",
    "            pretrained=False\n",
    "        )\n",
    "        model = model.to(DEVICE).eval()\n",
    "\n",
    "        model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "        models.append((model, runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [None for _ in range(len(models))]\n",
    "if any([runtime != \"torch\" for _, runtime in models]):\n",
    "    sessions = []\n",
    "\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    from onnxconverter_common import float16\n",
    "\n",
    "    input_names = ['x']\n",
    "    output_names = ['output']\n",
    "\n",
    "    input_tensor = torch.randn(\n",
    "        1 if EVAL else BATCH_SIZE,\n",
    "        config.n_channels,\n",
    "        config.melspec_config['n_mels'],\n",
    "        313 if config.melspec_config['hop_length'] == 512 else 224\n",
    "    )\n",
    "\n",
    "    for i, (model, runtime) in enumerate(models):\n",
    "        name = f\"model_{i}.onnx\"\n",
    "        torch.onnx.export(\n",
    "            model.encoder.cpu(),\n",
    "            input_tensor,\n",
    "            name,\n",
    "            verbose=False,\n",
    "            input_names=input_names,\n",
    "            output_names=output_names,\n",
    "        )\n",
    "        onnx_model = onnx.load(name)\n",
    "        # onnx_model = float16.convert_float_to_float16(onnx_model)\n",
    "        # onnx.save(onnx_model, f\"model_{i}.onnx\")\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        ort_session = ort.InferenceSession(f\"model_{i}.onnx\")\n",
    "\n",
    "        if runtime == \"onnx\":\n",
    "            sessions.append(ort_session)\n",
    "            print(f'- Convert model {name} to onnx')\n",
    "\n",
    "        elif runtime == \"openvino\":\n",
    "            import openvino.runtime as ov\n",
    "\n",
    "            !mo --input_model $name # --compress_to_fp16=False\n",
    "            \n",
    "            core = ov.Core()\n",
    "            openvino_model = core.read_model(model='model_0.xml')\n",
    "            compiled_model = core.compile_model(openvino_model, device_name=\"CPU\")\n",
    "            infer_request = compiled_model.create_infer_request()\n",
    "            sessions.append(infer_request)\n",
    "\n",
    "            print(f'- Convert model {name} to openvino')\n",
    "        else:\n",
    "            sessions.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    batches = np.array_split(np.arange(len(df)), len(df) / 1000)\n",
    "except:\n",
    "    batches = [np.arange(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_rows = []\n",
    "# for i, batch in enumerate(batches):\n",
    "#     print(f\"-> Batch {i + 1}/{len(batches)}\")\n",
    "#     df_batch = df.iloc[batch].reset_index(drop=True)\n",
    "\n",
    "#     waves = joblib.Parallel(n_jobs=os.cpu_count())(\n",
    "#         joblib.delayed(load_sample)(\n",
    "#             path, evaluate=EVAL, sr=SR, duration=DURATION, normalize=config.normalize\n",
    "#         )\n",
    "#         for path in tqdm(df_batch[\"path\"].values)\n",
    "#     )\n",
    "#     all_preds = [\n",
    "#         infer_sample(\n",
    "#             wave,\n",
    "#             models,\n",
    "#             sessions,\n",
    "#             device=DEVICE,\n",
    "#             use_fp16=USE_FP16,\n",
    "#         )\n",
    "#         for wave in tqdm(waves)\n",
    "#     ]\n",
    "\n",
    "#     del waves\n",
    "#     gc.collect()\n",
    "\n",
    "#     for idx in range(len(df_batch)):\n",
    "#         y_pred = all_preds[idx]\n",
    "#         preds = expit(y_pred).mean(0)\n",
    "\n",
    "#         for t, pred in enumerate(preds):\n",
    "#             predictions = dict([(l, p) for l, p in zip(CLASSES, pred)])\n",
    "#             inference_rows.append(\n",
    "#                 {\"row_id\": f\"{df_batch.id[idx]}_{(t + 1) * 5}\"} | predictions\n",
    "#             )\n",
    "\n",
    "#     del all_preds\n",
    "#     gc.collect()\n",
    "#     break\n",
    "\n",
    "# sub = pd.DataFrame(inference_rows)\n",
    "# if not EVAL:\n",
    "#     sub.to_csv(EXP_FOLDERS[0][0] + f'pl_sub_{model_idx}.csv', index=False)\n",
    "#     print('-> Saved predictions to', EXP_FOLDERS[0][0] + f'pl_sub_{model_idx}.csv')\n",
    "#     display(sub.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_rows = {i : [] for i in range(len(models))}\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    print(f\"-> Batch {i + 1}/{len(batches)}\")\n",
    "    df_batch = df.iloc[batch].reset_index(drop=True)\n",
    "\n",
    "    waves = joblib.Parallel(n_jobs=32)(\n",
    "        joblib.delayed(load_sample)(\n",
    "            path, evaluate=EVAL, sr=SR, duration=DURATION, normalize=config.normalize\n",
    "        )\n",
    "        for path in tqdm(df_batch[\"path\"].values)\n",
    "    )\n",
    "\n",
    "    for model_idx in range(len(models)):\n",
    "        all_preds = [\n",
    "            infer_sample(\n",
    "                wave,\n",
    "                [models[model_idx]],\n",
    "                sessions,\n",
    "                device=DEVICE,\n",
    "                use_fp16=USE_FP16,\n",
    "            )\n",
    "            for wave in tqdm(waves)\n",
    "        ]\n",
    "\n",
    "        for idx in range(len(df_batch)):\n",
    "            y_pred = all_preds[idx]\n",
    "            preds = expit(y_pred).mean(0)\n",
    "\n",
    "            for t, pred in enumerate(preds):\n",
    "                predictions = dict([(l, p) for l, p in zip(CLASSES, pred)])\n",
    "                inference_rows[model_idx].append(\n",
    "                    {\"row_id\": f\"{df_batch.id[idx]}_{(t + 1) * 5}\"} | predictions\n",
    "                )\n",
    "\n",
    "        del all_preds\n",
    "        gc.collect()\n",
    "    \n",
    "    del waves\n",
    "    gc.collect()\n",
    "\n",
    "    # break\n",
    "\n",
    "for model_idx in range(len(models)):\n",
    "    sub = pd.DataFrame(inference_rows[model_idx])\n",
    "    if not EVAL:\n",
    "        name = EXP_FOLDERS[0][0] + f'pl_sub_{EXP_FOLDERS[0][1][model_idx]}.csv'\n",
    "        sub.to_csv(name, index=False)\n",
    "        print('-> Saved predictions to', name)\n",
    "        display(sub.head())\n",
    "\n",
    "    del (sub, inference_rows[model_idx])\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot([\n",
    "#     0.6625, 0.7034, 0.8868, 0.8910, 0.9343, 0.9322, 0.9451, 0.9137, 0.9520, 0.9543, \n",
    "#     0.9637,0.9348, 0.9656, 0.9688, 0.9695, 0.9702, 0.9713, 0.9709, 0.9709, 0.9709,\n",
    "# ])\n",
    "# plt.grid()\n",
    "# plt.ylim(0.6, 1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL:\n",
    "    preds = sub[CLASSES].values\n",
    "    auc = macro_auc(df[\"primary_label\"].values.tolist()[:len(preds)], preds)\n",
    "    print(f'Fold 0 AUC: {auc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7241b2af102f7e024509099765066b36197b195077f7bfac6e5bc041ba17c8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
