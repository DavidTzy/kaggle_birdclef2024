{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to infer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "device = torch.cuda.get_device_name(0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import joblib\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.special import expit\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.logger import Config\n",
    "from util.metrics import macro_auc\n",
    "from util.torch import load_model_weights\n",
    "\n",
    "from data.preparation import prepare_data, prepare_folds\n",
    "from model_zoo.models import define_model\n",
    "from inference.predict import infer_onnx, load_sample, infer_sample\n",
    "from params import CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL:\n",
    "    DATA_PATH = \"../input/train_audio/\"\n",
    "else:\n",
    "    DATA_PATH = \"../input/unlabeled_soundscapes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "USE_FP16 = True\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "DURATION = 5\n",
    "SR = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0 if EVAL else \"fullfit_0\"\n",
    "\n",
    "EXP_FOLDERS = [\n",
    " \n",
    "    # (\"../logs/2024-05-12/1/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # effnet-b0 PL0.72\n",
    "    # (\"../logs/2024-05-12/2/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b1 PL0.72\n",
    "    # (\"../logs/2024-05-13/0/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b0 PLBirdnet\n",
    "    # (\"../logs/2024-05-13/3/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b0 PL0.72 cpmp params\n",
    "    # (\"../logs/2024-05-14/0/\", [f\"fullfit_{i}\" for i in range(5)], \"torch\"),  # vit-b0 PL0.72 cpmp params default head  <- LB 0.7\n",
    "\n",
    "    # # ROUND 1\n",
    "    # (\"../logs/2024-05-14/22/\", [f\"{i}\" for i in range(4)], \"torch\"),  # vit-b0 PL all above             0.69\n",
    "    # (\"../logs/2024-05-15/0/\", [f\"{i}\" for i in range(4)], \"torch\"),   # vit-b0 PL all above more augs   0.65\n",
    "    # (\"../logs/2024-05-15/1/\", [f\"{i}\" for i in range(4)], \"torch\"),   # vit-b0 PL all above no augs     0.70  <--\n",
    "    # (\"../logs/2024-05-15/2/\", [f\"{i}\" for i in range(4)], \"torch\"),   # vit-b0 PL all above fix mix     0.69\n",
    "\n",
    "    # (\"../logs/2024-05-16/1/\", [f\"{i}\" for i in range(4)], \"torch\"),  # vit-b0 PL all above less augs\n",
    "\n",
    "    # # ROUND 0 - Mixup only\n",
    "    # (\"../logs/2024-05-16/2/\", [f\"{i}\" for i in range(4)], \"torch\"),   # vit-b0\n",
    "    # (\"../logs/2024-05-16/3/\", [f\"{i}\" for i in range(4)], \"torch\"),   # vit-b1\n",
    "    # (\"../logs/2024-05-16/4/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mixnet\n",
    "    # (\"../logs/2024-05-16/5/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mobilenet\n",
    "    # (\"../logs/2024-05-16/6/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mnasnet\n",
    "    # (\"../logs/2024-05-16/7/\", [f\"{i}\" for i in range(4)], \"torch\"),   # b0\n",
    "    # (\"../logs/2024-05-16/8/\", [f\"{i}\" for i in range(4)], \"torch\"),   # tinynet\n",
    "    # (\"../logs/2024-05-16/9/\", [f\"{i}\" for i in range(4)], \"torch\"),   # b0-v2\n",
    "\n",
    "    # ROUND 1 - \n",
    "    (\"../logs/2024-05-16/12/\", [f\"{i}\" for i in range(4)], \"torch\"),   # vit-b0 PL2 no augs             0.71\n",
    "    # (\"../logs/2024-05-16/14/\", [f\"{i}\" for i in range(4)], \"torch\"),   # vit-b0 PL2 no overfitnoisy   0.71\n",
    "\n",
    "    # (\"../logs/2024-05-17/0/\", [f\"{i}\" for i in range(4)], \"torch\"),   # v2-b0 PL2 no augs     \n",
    "    # (\"../logs/2024-05-17/1/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mixnet PL2 no augs     \n",
    "    (\"../logs/2024-05-17/2/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mobilenetv2 PL2 no augs         0.69\n",
    "    (\"../logs/2024-05-17/3/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mnasnet PL2 no augs             0.70\n",
    "\n",
    "    # (\"../logs/2024-05-17/4/\", [f\"{i}\" for i in range(4)], \"torch\"),   # b0 PL2 no augs\n",
    "    # (\"../logs/2024-05-17/5/\", [f\"{i}\" for i in range(4)], \"torch\"),   # tinynet PL2 no augs\n",
    "    # (\"../logs/2024-05-17/8/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm0 PL2 no augs\n",
    "    # (\"../logs/2024-05-17/9/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm1 PL2 no augs\n",
    "    (\"../logs/2024-05-18/0/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm2 PL2 no augs            0.69\n",
    "    (\"../logs/2024-05-18/1/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm3 PL2 no augs            0.70\n",
    "    # (\"../logs/2024-05-18/4/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mnasnet PL2 no augs 20 eps    0.67\n",
    "    (\"../logs/2024-05-18/8/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mobilenetv3_s PL2 no augs       0.??\n",
    "    (\"../logs/2024-05-19/0/\", [f\"{i}\" for i in range(4)], \"torch\"),   # efficientnet_lite0 PL2 no augs  0.70\n",
    "    (\"../logs/2024-05-19/1/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm4 PL2 no augs            0.??\n",
    "    # (\"../logs/2024-05-19/2/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm5 PL2 no augs            0.69\n",
    "\n",
    "    # (\"../logs/2024-05-19/6/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm3 PL2 no augs 16/48 pl   0.70\n",
    "\n",
    "    (\"../logs/2024-05-19/9/\", [f\"{i}\" for i in range(4)], \"torch\"),   # regnety_002 PL2 no augs\n",
    "    (\"../logs/2024-05-19/10/\", [f\"{i}\" for i in range(4)], \"torch\"),  # regnety_004 PL2 no augs\n",
    "    (\"../logs/2024-05-20/1/\", [f\"{i}\" for i in range(4)], \"torch\"),   # lcnet_100 PL2 no augs\n",
    "    (\"../logs/2024-05-20/2/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mobilenetv3_lm PL2 no augs\n",
    "    (\"../logs/2024-05-20/5/\", [f\"{i}\" for i in range(4)], \"torch\"),   # repghostnet_100 PL2 no augs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0 if EVAL else \"fullfit_0\"\n",
    "\n",
    "EXP_FOLDERS = [\n",
    "    # ROUND 1 - \n",
    "    # (\"../logs/2024-05-16/12/\", [f\"{i}\" for i in range(4)], \"torch\"),  # vit-b0 PL2 no augs              0.71\n",
    "    # (\"../logs/2024-05-17/2/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mobilenetv2 PL2 no augs         0.69\n",
    "    # (\"../logs/2024-05-17/3/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mnasnet PL2 no augs             0.70\n",
    "\n",
    "    # (\"../logs/2024-05-18/1/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm3 PL2 no augs            0.70\n",
    "    # (\"../logs/2024-05-18/8/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mobilenetv3_s PL2 no augs       0.??\n",
    "    # (\"../logs/2024-05-19/0/\", [f\"{i}\" for i in range(4)], \"torch\"),   # efficientnet_lite0 PL2 no augs  0.70\n",
    "    # (\"../logs/2024-05-19/1/\", [f\"{i}\" for i in range(4)], \"torch\"),   # effvitm4 PL2 no augs            0.??\n",
    "\n",
    "    (\"../logs/2024-05-19/9/\", [f\"{i}\" for i in range(4)], \"torch\"),   # regnety_002 PL2 no augs\n",
    "    # (\"../logs/2024-05-19/10/\", [f\"{i}\" for i in range(4)], \"torch\"),  # regnety_004 PL2 no augs\n",
    "    # (\"../logs/2024-05-20/1/\", [f\"{i}\" for i in range(4)], \"torch\"),   # lcnet_100 PL2 no augs\n",
    "    (\"../logs/2024-05-20/2/\", [f\"{i}\" for i in range(4)], \"torch\"),   # mobilenetv3_lm PL2 no augs\n",
    "    # (\"../logs/2024-05-20/5/\", [f\"{i}\" for i in range(4)], \"torch\"),   # repghostnet_100 PL2 no augs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util.logger import upload_to_kaggle\n",
    "\n",
    "# upload_to_kaggle(\n",
    "#     [f[0] for f in EXP_FOLDERS],\n",
    "#     directory=\"../output/dataset_1/\",\n",
    "#     dataset_name=\"BirdCLEF 2024 Weights 1\",\n",
    "#     update_folders=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL:\n",
    "    df = pd.DataFrame({\"path\": glob.glob(DATA_PATH + \"*/*\")})\n",
    "    df[\"id\"] = df[\"path\"].apply(lambda x: x.split(\"/\")[-1][:-4])\n",
    "\n",
    "    folds = pd.read_csv('../input/folds_4.csv')\n",
    "    folds['id'] = folds['filename'].apply(lambda x: x.split('/')[-1][:-4])\n",
    "    df = df.merge(folds)\n",
    "    df = df[df['fold'] == 0].reset_index(drop=True)\n",
    "\n",
    "    df[\"primary_label\"] = df[\"path\"].apply(lambda x:  x.split('/')[-2])\n",
    "\n",
    "    cts = df[\"primary_label\"].value_counts()\n",
    "    low_rep = list(cts[cts < 50].index)\n",
    "    df = df[df[\"primary_label\"].isin(low_rep)].reset_index(drop=True)\n",
    "else:\n",
    "    df = pd.DataFrame({\"path\": glob.glob(DATA_PATH + \"*\")})\n",
    "    df[\"id\"] = df[\"path\"].apply(lambda x: x.split(\"/\")[-1][:-4])\n",
    "    \n",
    "    # df[\"duration\"] = df[\"path\"].apply(lambda x: librosa.get_duration(path=x))\n",
    "    # df = df[df[\"duration\"] == 240].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for e in EXP_FOLDERS:\n",
    "    try:\n",
    "        exp_folder, folds, runtime = e\n",
    "    except:\n",
    "        exp_folder, folds = e\n",
    "        runtime = \"torch\"\n",
    "    \n",
    "    config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "\n",
    "    for fold in folds:\n",
    "        weights = exp_folder + f\"{config.name}_{fold}.pt\"\n",
    "\n",
    "        model = define_model(\n",
    "            config.name,\n",
    "            config.melspec_config,\n",
    "            head=config.head,\n",
    "            aug_config=config.aug_config,\n",
    "            num_classes=config.num_classes,\n",
    "            n_channels=config.n_channels,\n",
    "            drop_rate=config.drop_rate,\n",
    "            drop_path_rate=config.drop_path_rate,\n",
    "            norm=config.norm if hasattr(config, \"norm\") else \"min_max\",\n",
    "            top_db=config.top_db if hasattr(config, \"top_db\") else None,\n",
    "            exportable=config.exportable,\n",
    "            verbose=True,\n",
    "            pretrained=False\n",
    "        )\n",
    "        model = model.to(DEVICE).eval()\n",
    "\n",
    "        model = load_model_weights(model, weights, verbose=config.local_rank == 0)\n",
    "        \n",
    "        models.append((model, runtime, (e[0], fold)))\n",
    "\n",
    "        if EVAL:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [None for _ in range(len(models))]\n",
    "if any([runtime != \"torch\" for _, runtime, _ in models]):\n",
    "    sessions = []\n",
    "\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    from onnxconverter_common import float16\n",
    "\n",
    "    input_names = ['x']\n",
    "    output_names = ['output']\n",
    "\n",
    "    input_tensor = torch.randn(\n",
    "        1 if EVAL else BATCH_SIZE,\n",
    "        config.n_channels,\n",
    "        config.melspec_config['n_mels'],\n",
    "        313 if config.melspec_config['hop_length'] == 512 else 224\n",
    "    )\n",
    "\n",
    "    for i, (model, runtime, name) in enumerate(models):\n",
    "        name = f\"model_{i}.onnx\"\n",
    "        torch.onnx.export(\n",
    "            model.encoder.cpu(),\n",
    "            input_tensor,\n",
    "            name,\n",
    "            verbose=False,\n",
    "            input_names=input_names,\n",
    "            output_names=output_names,\n",
    "        )\n",
    "        onnx_model = onnx.load(name)\n",
    "        # onnx_model = float16.convert_float_to_float16(onnx_model)\n",
    "        # onnx.save(onnx_model, f\"model_{i}.onnx\")\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        ort_session = ort.InferenceSession(f\"model_{i}.onnx\")\n",
    "\n",
    "        if runtime == \"onnx\":\n",
    "            sessions.append(ort_session)\n",
    "            print(f'- Convert model {name} to onnx')\n",
    "\n",
    "        elif runtime == \"openvino\":\n",
    "            import openvino.runtime as ov\n",
    "\n",
    "            !mo --input_model $name # --compress_to_fp16=False\n",
    "            \n",
    "            core = ov.Core()\n",
    "            openvino_model = core.read_model(model='model_0.xml')\n",
    "            compiled_model = core.compile_model(openvino_model, device_name=\"CPU\")\n",
    "            infer_request = compiled_model.create_infer_request()\n",
    "            sessions.append(infer_request)\n",
    "\n",
    "            print(f'- Convert model {name} to openvino')\n",
    "        else:\n",
    "            sessions.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    batches = np.array_split(np.arange(len(df)), len(df) / 100)\n",
    "except:\n",
    "    batches = [np.arange(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_rows = {i : [] for i in range(len(models))}\n",
    "\n",
    "for i, batch in enumerate(tqdm(batches)):\n",
    "    df_batch = df.iloc[batch].reset_index(drop=True)\n",
    "\n",
    "    waves = joblib.Parallel(n_jobs=4)(\n",
    "        joblib.delayed(load_sample)(\n",
    "            path, evaluate=EVAL, sr=SR, duration=DURATION, normalize=config.wav_norm\n",
    "        )\n",
    "        for path in df_batch[\"path\"].values\n",
    "    )\n",
    "\n",
    "    for model_idx in range(len(models)):\n",
    "        all_preds = [\n",
    "            infer_sample(\n",
    "                wave,\n",
    "                [models[model_idx][:2]],\n",
    "                sessions,\n",
    "                device=DEVICE,\n",
    "                use_fp16=USE_FP16,\n",
    "            )\n",
    "            for wave in waves\n",
    "        ]\n",
    "\n",
    "        for idx in range(len(df_batch)):\n",
    "            y_pred = all_preds[idx]\n",
    "            preds = expit(y_pred).mean(0)\n",
    "\n",
    "            for t, pred in enumerate(preds):\n",
    "                predictions = dict([(l, p) for l, p in zip(CLASSES, pred)])\n",
    "                inference_rows[model_idx].append(\n",
    "                    {\"row_id\": f\"{df_batch.id[idx]}_{(t + 1) * 5}\"} | predictions\n",
    "                )\n",
    "\n",
    "        del all_preds\n",
    "        gc.collect()\n",
    "    \n",
    "    del waves\n",
    "    gc.collect()\n",
    "\n",
    "    # break\n",
    "\n",
    "subs = {}\n",
    "for model_idx in range(len(models)):\n",
    "    sub = pd.DataFrame(inference_rows[model_idx])\n",
    "    name = models[model_idx][2][0] + f'pl_preds_{models[model_idx][2][1]}'\n",
    "\n",
    "    if not EVAL:\n",
    "        sub[[\"row_id\"]].to_csv(name + \".csv\", index=False)\n",
    "        np.save(name + \".npy\", sub[CLASSES].values)\n",
    "\n",
    "        print(f\"-> Saved predictions to {name}[.csv/.npy]\")\n",
    "        display(sub.head(2))\n",
    "\n",
    "        del (sub, inference_rows[model_idx])\n",
    "    else:\n",
    "        subs[name] = sub\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL:\n",
    "    for k in subs.keys():\n",
    "        \n",
    "        exp = k.rsplit('/', 1)[0]\n",
    "        config = Config(json.load(open(exp + \"/config.json\", \"r\")))\n",
    "        model = config.name\n",
    "\n",
    "        print(f\"\\n -> Model {exp} - {model} - PL {config.use_pl} \\n\")\n",
    "\n",
    "        sub = subs[k]\n",
    "        preds = sub[CLASSES].values\n",
    "        max_ = preds.max(1)\n",
    "        auc, aucs = macro_auc(\n",
    "            df[\"primary_label\"].values.tolist()[: len(preds)],\n",
    "            preds,\n",
    "            return_per_class=True\n",
    "        )\n",
    "\n",
    "        print(f\"Fold 0 AUC: {auc:.5f}\\n\")\n",
    "\n",
    "        df_auc = pd.DataFrame({\"auc\": np.clip(list(aucs.values()), 0.5, 1), \"max\": preds.max(0)}, index=aucs.keys())\n",
    "        df_auc[\"count\"] = df_auc.index.map(cts)\n",
    "\n",
    "        df_auc = df_auc[df_auc.index.isin(low_rep)]\n",
    "\n",
    "        df_auc = df_auc.sort_values('auc').head(50)\n",
    "        display(df_auc.T)\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.histplot(df_auc['auc'], bins=25, kde=True)\n",
    "        plt.xlim(0.5, 1)\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.histplot(df_auc['max'], bins=25, kde=True)\n",
    "        plt.xlim(0., 1)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "\n",
    "        plt.scatter(df_auc[\"max\"], df_auc[\"auc\"], s=5)\n",
    "        plt.plot([-1, 2], [0, 1.5], c=\"salmon\")\n",
    "        plt.xlim(-0.01, 1.01)\n",
    "        plt.ylim(0.49, 1.01)\n",
    "\n",
    "        plt.show()\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7241b2af102f7e024509099765066b36197b195077f7bfac6e5bc041ba17c8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
